{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.12rc1 (v2.7.12rc1:13912cd1e7e8, Jun 11 2016, 15:32:34) \n",
      "[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SENG474-HD\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load('/Users/Matthew/Downloads/data_Q1_2017/2017-01-01.csv', \n",
    "                     format='com.databricks.spark.csv', \n",
    "                     header='true', \n",
    "                     inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Number of data points:\", str(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'serial_number',\n",
       " 'model',\n",
       " 'capacity_bytes',\n",
       " 'failure',\n",
       " 'smart_1_normalized',\n",
       " 'smart_1_raw',\n",
       " 'smart_2_normalized',\n",
       " 'smart_2_raw',\n",
       " 'smart_3_normalized',\n",
       " 'smart_3_raw',\n",
       " 'smart_4_normalized',\n",
       " 'smart_4_raw',\n",
       " 'smart_5_normalized',\n",
       " 'smart_5_raw',\n",
       " 'smart_7_normalized',\n",
       " 'smart_7_raw',\n",
       " 'smart_8_normalized',\n",
       " 'smart_8_raw',\n",
       " 'smart_9_normalized',\n",
       " 'smart_9_raw',\n",
       " 'smart_10_normalized',\n",
       " 'smart_10_raw',\n",
       " 'smart_11_normalized',\n",
       " 'smart_11_raw',\n",
       " 'smart_12_normalized',\n",
       " 'smart_12_raw',\n",
       " 'smart_13_normalized',\n",
       " 'smart_13_raw',\n",
       " 'smart_15_normalized',\n",
       " 'smart_15_raw',\n",
       " 'smart_22_normalized',\n",
       " 'smart_22_raw',\n",
       " 'smart_183_normalized',\n",
       " 'smart_183_raw',\n",
       " 'smart_184_normalized',\n",
       " 'smart_184_raw',\n",
       " 'smart_187_normalized',\n",
       " 'smart_187_raw',\n",
       " 'smart_188_normalized',\n",
       " 'smart_188_raw',\n",
       " 'smart_189_normalized',\n",
       " 'smart_189_raw',\n",
       " 'smart_190_normalized',\n",
       " 'smart_190_raw',\n",
       " 'smart_191_normalized',\n",
       " 'smart_191_raw',\n",
       " 'smart_192_normalized',\n",
       " 'smart_192_raw',\n",
       " 'smart_193_normalized',\n",
       " 'smart_193_raw',\n",
       " 'smart_194_normalized',\n",
       " 'smart_194_raw',\n",
       " 'smart_195_normalized',\n",
       " 'smart_195_raw',\n",
       " 'smart_196_normalized',\n",
       " 'smart_196_raw',\n",
       " 'smart_197_normalized',\n",
       " 'smart_197_raw',\n",
       " 'smart_198_normalized',\n",
       " 'smart_198_raw',\n",
       " 'smart_199_normalized',\n",
       " 'smart_199_raw',\n",
       " 'smart_200_normalized',\n",
       " 'smart_200_raw',\n",
       " 'smart_201_normalized',\n",
       " 'smart_201_raw',\n",
       " 'smart_220_normalized',\n",
       " 'smart_220_raw',\n",
       " 'smart_222_normalized',\n",
       " 'smart_222_raw',\n",
       " 'smart_223_normalized',\n",
       " 'smart_223_raw',\n",
       " 'smart_224_normalized',\n",
       " 'smart_224_raw',\n",
       " 'smart_225_normalized',\n",
       " 'smart_225_raw',\n",
       " 'smart_226_normalized',\n",
       " 'smart_226_raw',\n",
       " 'smart_240_normalized',\n",
       " 'smart_240_raw',\n",
       " 'smart_241_normalized',\n",
       " 'smart_241_raw',\n",
       " 'smart_242_normalized',\n",
       " 'smart_242_raw',\n",
       " 'smart_250_normalized',\n",
       " 'smart_250_raw',\n",
       " 'smart_251_normalized',\n",
       " 'smart_251_raw',\n",
       " 'smart_252_normalized',\n",
       " 'smart_252_raw',\n",
       " 'smart_254_normalized',\n",
       " 'smart_254_raw',\n",
       " 'smart_255_normalized',\n",
       " 'smart_255_raw']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.show(10, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def transform_data(data):\n",
    "    prediction_col = \"failure\"\n",
    "    columns = []#[\"serial_number\", \"model\", \"capacity_bytes\"]\n",
    "    # Add Smart columns.\n",
    "    for i in range(1, 255):\n",
    "        test_col = \"smart_\" + str(i) + \"_normalized\"\n",
    "        try:\n",
    "            data[test_col]\n",
    "            columns.append(test_col)\n",
    "        except:\n",
    "            pass\n",
    "    return LabeledPoint(float(data[prediction_col]), [0.0 if data[i] is None else data[i] for i in columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.rdd.map(transform_data)\n",
    "#print(df_model.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = df_model.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "#dt = DecisionTree.trainClassifier(trainingData, numClasses=2,categoricalFeaturesInfo={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = dt.predict(testData.map(lambda x: x.features))\n",
    "#labels_and_predictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "#testErr = float(labels_and_predictions.filter(lambda lp: lp[0] != lp[1]).count()) / float(testData.count())\n",
    "#print(\"Test Error: \", str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "def transform_data2(data):\n",
    "    prediction_col = \"failure\"\n",
    "    columns = [] #[\"serial_number\", \"model\", \"capacity_bytes\"]\n",
    "    # Add Smart columns.\n",
    "    \n",
    "    if data[prediction_col] is None:\n",
    "        print(data)\n",
    "    \n",
    "    for i in range(1, 255):\n",
    "        test_col = \"smart_\" + str(i) + \"_normalized\"\n",
    "        try:\n",
    "            data[test_col]\n",
    "            columns.append(test_col)\n",
    "        except:\n",
    "            pass\n",
    "    return (float(data[prediction_col]), Vectors.dense(*[data[i] for i in columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[100.0,135.0,127....|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#labels_and_predictions.show(10, False)\n",
    "df.limit(1).fillna(0.0).rdd.map(transform_data2).toDF([\"label\", \"features\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "(trainingData, testData) = df.fillna(0.0).rdd.map(transform_data2).toDF([\"label\", \"features\"]).randomSplit([0.7, 0.3])\n",
    "numFolds = 5\n",
    "\n",
    "rf = RandomForestClassifier(maxMemoryInMB=1024)\n",
    "evaluator = MulticlassClassificationEvaluator() # + other params as in Scala\n",
    "\n",
    "#.addGrid(rf.impurity, [\"variance\", \"gini\"])\\\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth, [2, 4, 6, 8, 10]) \\\n",
    "    .addGrid(rf.numTrees, [1, 10, 20, 30])\\\n",
    "    .build()\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=numFolds)\n",
    "\n",
    "model = crossval.fit(trainingData)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labels_and_predictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = float(labels_and_predictions.filter(lambda lp: lp[0] != lp[1]).count()) / float(testData.count())\n",
    "print(\"Random ForestTest Error: \", str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
